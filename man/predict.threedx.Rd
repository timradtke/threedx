% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/predict.R
\name{predict.threedx}
\alias{predict.threedx}
\title{Draw forecast sample paths from a fitted 3DX model}
\usage{
\method{predict}{threedx}(
  object,
  horizon,
  n_samples,
  observation_driven,
  innovation_function,
  postprocess = identity,
  ...
)
}
\arguments{
\item{object}{A fitted model object of class \code{threedx}}

\item{horizon}{An integer defining the forecast horizon}

\item{n_samples}{An integer defining the number of sample paths to draw}

\item{observation_driven}{Logical; if \code{TRUE}, sample paths are generated by
drawing from weighted historical observations directly instead of adding
innovation noise on the current mean forecast. This is similar to the
Non-Parametric Time Series forecaster (NPTS) described in
"GluonTS: Probabilistic Time Series Models in Python" (2019) by Flunkert et
al. (see https://arxiv.org/abs/1906.05264). If \code{FALSE}, sample paths will
be generated by adding innovation noise on top of the current weighted
average forecast.}

\item{innovation_function}{A function with arguments \code{n} and \code{errors}. Must
be able to handle additional parameters via \code{...} to allow for potential
future changes in the set of arguments passed to
\code{innovation_function} by \code{\link[=predict.threedx]{predict.threedx()}}. For examples, see
\code{\link[=draw_normal_with_drift]{draw_normal_with_drift()}} or \code{\link[=draw_bootstrap_weighted]{draw_bootstrap_weighted()}}.
The provided \code{innovation_function} must return a numeric vector of length
\code{n} that contains i.i.d samples that can be used for any sample path and
forecast horizon.
This argument is ignored when \code{observation_driven=TRUE}.}

\item{postprocess}{A function that is applied on a numeric matrix of
drawn samples for a single step-ahead before the samples are used to
update the state of the model, and before outliers are removed
(if applicable). By default equal to \code{identity()}, but could also
be something like \code{function(x) pmax(x, 0)} to enforce a lower bound of 0,
or any other transformation of interest that returns a numeric matrix of
same dimensions as those of the input.
Note that this can cause arbitrary errors caused by the author of the
function provided to \code{postprocess}.}

\item{...}{Additional arguments passed to \code{innovation_function}}
}
\value{
A forecast object of class \code{threedx_paths}, which is a list of:
\itemize{
\item A \code{paths} numeric matrix of dimensions \code{n_samples}-times-\code{horizon}. The
matrix stores the forecast sample paths that are the probabilistic forecast
returned by the \code{threedx} model. Aggregate across the first dimension to
derive expectations of the forecast distribution, aggregate across the
second dimension to derive sums over the forecast horizon.
\item The input \code{horizon} argument
\item The input \code{n_samples} argument
\item The input \code{observation_driven} argument
\item The model \code{model} provided via \code{object}, an object of class \code{threedx}
}
}
\description{
Draw forecast sample paths from a fitted 3DX model
}
\details{
While \code{observation_driven} can be effective at forecasting wide
ranges of time series, the induced forecast distribution collapses to a
single point as \code{alpha}, \code{alpha_seasonal}, and \code{alpha_seasonal_decay}
values tend towards 1. While the corresponding point prediction may still
be optimal, the uncertainty in the forecast is likely underestimated.

For example, if \code{alpha = 0.98}, \code{alpha_seasonal=0}, and
\code{alpha_seasonal_decay=0}, the resulting forecast distribution assigns 98\%
probability to the most recent observation, thus collapsing most prediction
intervals onto that point. This can be highly unrealistic for time series
whose level is changing rapidly and therefore require large \code{alpha} values.

To circumvent this issue, one can design certain conditions under
which \code{observation_driven} is set to \code{TRUE}. For example, using
\code{\link[=k_largest_weights_sum_to_less_than_p_percent]{k_largest_weights_sum_to_less_than_p_percent()}}, one can check that the
learned model assigns weight to at least a few observations, and only then
set \code{observation_driven=TRUE}. However, if the underlying time series is
truly a random walk, \code{observation_driven=TRUE} will always be a poor choice
as it limits the range of possible future values too much.
}
\examples{
set.seed(9284)
y <- stats::rpois(n = 55, lambda = pmax(0.1, 1 + 10 * sinpi((5 + 1:55 )/ 6)))

model <- learn_weights(
  y = y,
  alphas_grid = list_sampled_alphas(
    n_target = 1000L,
    include_edge_cases = TRUE
  ),
  period_length = 12L,
  loss_function = loss_mae
)

forecast_observation_driven <- predict(
  object = model,
  horizon = 12L,
  n_samples = 2500L,
  observation_driven = TRUE
)

if (require("ggplot2")) {
  autoplot(forecast_observation_driven)
}

forecast_latent <- predict(
  object = model,
  horizon = 12L,
  n_samples = 2500L,
  observation_driven = FALSE,
  innovation_function = draw_bootstrap
)

if (require("ggplot2")) {
  autoplot(forecast_latent)
}

forecast_latent_with_postprocessing <- predict(
  object = model,
  horizon = 12L,
  n_samples = 2500L,
  observation_driven = FALSE,
  innovation_function = draw_normal_with_zero_mean,
  postprocess = function(x) round(pmax(x, 0))
)

if (require("ggplot2")) {
  autoplot(forecast_latent_with_postprocessing)
}

}
\seealso{
\code{\link[=learn_weights]{learn_weights()}}, \code{\link[=k_largest_weights_sum_to_less_than_p_percent]{k_largest_weights_sum_to_less_than_p_percent()}},
\code{\link[=loss_mae]{loss_mae()}}, \code{\link[=loss_rmse]{loss_rmse()}}, \code{\link[=draw_normal_with_zero_mean]{draw_normal_with_zero_mean()}},
\code{\link[=draw_bootstrap]{draw_bootstrap()}}
}
